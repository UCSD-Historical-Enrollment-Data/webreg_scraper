<p align="center">
  <img src="https://github.com/ewang2002/webreg_scraper/blob/master/assets/project_banner.png?raw=true"  alt="Project Banner"/>
</p>

<p align="center">
  <a href="https://github.com/ewang2002/webweg">webweg</a> |
  <b>webreg_scraper</b> |
  <a href="https://github.com/ewang2002/UCSDHistEnrollData">UCSDHistEnrollmentData</a>
</p>

A program designed to both scrape WebReg for enrollment data as well as make WebReg data available to other applications.

# Table of contents

- [Features](#features)
- [Helpers](#helpers)
- [Scraper Configuration File](#scraper-configuration-file)
    - [Base](#base)
    - [Base → API Info / Recovery Info](#base--api-info--recovery-info)
    - [Base → Wrapper Data](#base--wrapper-data)
    - [Base → Wrapper Data → Search Query](#base--wrapper-data--search-query)
- [Sample Configuration File](#sample-configuration-file)
- [Setup](#setup)
    - [Part 1: `webregautoin`](#part-1-webregautoin)
    - [Part 2: The Scraper](#part-2-the-scraper)
- [Implementation](#implementation)
    - [The Idea](#the-idea)
    - [Authentication](#authentication)
    - [The Challenge](#the-challenge)
- [License](#license)

Generated by [this website](https://luciopaiva.com/markdown-toc/).

## Helper Scripts
Aside from the scraper itself, there are two other helper programs -- one of which is essentially required.

| Program | Information |
| ------- |-------------|
| [`webregautoin`](https://github.com/ewang2002/webreg_scraper/tree/master/webregautoin) | A basic web server designed to automatically log the scraper into WebReg. **This is required.** |
| [`notifierbot`](https://github.com/ewang2002/webreg_scraper/tree/master/notifierbot) | A simple Discord Bot that notifies you if the scraper is not working. |

This program _requires_ the `webregautoin` helper program. 

## Scraper Configuration File
Below, you'll get an idea of what the configuration file should look like. All entries are required.

| Key | Type | Information |
| --- | ---- | ----------- |
| `configName` | `string` | The name of the configuration file. This is only used for identification purposes. |
| `apiBaseEndpoint` | `object` | Hosting information for the web server for the API. See **API Info / Recovery Info** for associated entries. |
| `cookieServer` | `object` | The address to the web server that the scraper can use to log back into WebReg if it gets logged out. See **API Info / Recovery Info** for more information. This relies on [`webregautoin`](https://github.com/ewang2002/webreg_scraper/tree/master/webregautoin).  |
| `verbose` | `boolean` | Whether logging should be verbose. |
| `wrapperData` | `object[]` | An array of objects representing each term that the scraper should consider. See **Wrapper Data** for associated entries. |

### Base → API Info / Recovery Info
All entries below are under `apiBaseEndpoint`.

| Key | Type | Information |
| --- | ---- | ----------- |
| `address` | `string` | The web server's address. |
| `port` | `number` | The web server's port. |

### Base → Wrapper Data
All entries below are under `wrapperData`.

| Key | Type | Information |
| --- | ---- | ----------- |
| `term` | `string` | The four character term that the scraper should consider. The first two characters must be one of `FA`, `WI`, `SP`, `S1`, `S2`, `S3` and the last two characters must be an integer representing the year. For example, `SP24` represents the `Spring 2024` term. |
| `cooldown` | `number` | The cooldown between requests, in seconds. |
| `searchQuery` | `object[]` | The courses to search and gather data for. See **Search Query** for associated entries. |
| `saveDataToFile` | `boolean` | Whether the data scraped for this term is actually saved. |

### Base → Wrapper Data → Search Query
All entries below are under `wrapperData[n].searchQuery`, where `n` is some integer used to index the array.

| Key | Type | Information |
| --- | ---- | ----------- |
| `levels` | `string[]` | The course levels. This can either be `g` (graduate), `u` (upper-division), or `l` (lower-division) |
| `departments` | `string[]` | All departments to consider. All elements here must be the department's code (e.g., for all courses under the History department, use `HIST`). An empty array indicates that all departments should be considered. |

## Sample Configuration File
The following configuration file (also found in `config.example.json`) defines the following:
- Simply named **Production Configuration**.
- Create a web server on `0.0.0.0:3000`.
- Defines the cookie server to be on `127.0.0.1:3001`.
- Allows verbose logging.
- Defines one term (`FA23`, or Fall 2023) with the following information:
    - After a request, wait 0.2 seconds before making another request.
    - Scrape the following courses:
        - All graduate MATH, CSE, COGS, and ECE courses.
        - All lower-division and upper-division courses.
    - Saves all data to a file.

```json
{
  "configName": "Production Configuration",
  "apiBaseEndpoint": {
    "address": "0.0.0.0",
    "port": 3000
  },
  "cookieServer": {
    "address": "127.0.0.1",
    "port": 3001
  },
  "verbose": true,
  "wrapperData": [
    {
      "term": "FA23",
      "cooldown": 0.2,
      "searchQuery": [
        {
          "levels": [
            "l",
            "u",
            "g"
          ],
          "departments": ["MATH", "COGS", "CSE", "ECE"]
        }
      ],
      "saveDataToFile": true
    }
  ]
}
```

## Setup
Below are instructions on how you can run this project. This setup guide assumes the use of Ubuntu for the host operating system, although other Linux distributions should be relatively similar.

### Part 0: Prerequisites
- Install the latest version of [Node.js](https://nodejs.org/en/).
- You'll also need to install the following system dependencies so that the headless browser can be used.
  ```
  sudo apt-get update
  sudo apt install libgtk-3-dev libnotify-dev libgconf-2-4 libnss3 libxss1 libasound2
  ```
  This was taken from [here](https://github.com/puppeteer/puppeteer/blob/main/docs/troubleshooting.md#running-puppeteer-on-wsl-windows-subsystem-for-linux).

### Part 1: `webregautoin`
As mentioned above, `webregautoin` is the script that allows the scraper to actually log into WebReg.
1. A sample configuration file has been provided for you; this file is called [`credentials.example.json`](https://github.com/ewang2002/webreg_scraper/blob/master/webregautoin/credentials.sample.json). 
   1. Rename this file to `credentials.json`. 
   2. Open the file and fill in your UC San Diego Active Directory username and password. 
   3. Save your changes.

2. Next, install TypeScript globally:
    ```
    npm i -g typescript 
    ```
    As the script is written in TypeScript, you need to install TypeScript to "compile" the script.

3. Install the dependencies that this script needs using the command:
    ```
    npm i
    ```

4. Run the following command to compile the script:
    ```
    npm run compile
    ```
    This is an alias for the command `tsc -p .`, which is defined in the `package.json` file.

5. At this point, you should see an `out` folder. In the `out` folder, you should have an `index.js` file. You can 
   run the command like so:
    ```
    node index.js --port <port>
    ```
   where
    - `port` is the port where this script should be "visible" to the scraper. Usually, I put a number like `3001` or
      `4000`.

Keep in mind that you'll need to redo this process once every 6-7 days, since Duo only remembers you for up to 7 days. 

### Part 2: The Scraper

Please follow the directions based on what you want. 

<details>
<summary>Just Want Executable.</summary>
<br> 

If you just want the executable, download the [relevant executable here](https://github.com/ewang2002/webreg_scraper/releases). The executables are in a zipped file, so you'll need to unzip it. The naming scheme of the file is `webreg_scraper-x86_64-[operating system].[extension]`.

After you get the relevant executable, please finish step 3 and the later part of step 5 under "Self-Compiling." 

</details>


<details>
<summary>Self-Compiling.</summary>
<br> 

1. Install the required system dependencies:
    ```
    sudo apt-get update
    sudo apt install build-essential
    sudo apt install pkg-config
    sudo apt install libssl-dev
    ```
    You'll also need to install [Rust](https://www.rust-lang.org/tools/install).

2. Download the contents of this repository. You can either download it directly, or use something like `git clone`.

3. A sample configuration file has been provided for you; this file is called [`config.example.json`](https://github.com/ewang2002/webreg_scraper/blob/master/config.example.json). 
   1. Rename this file to `config.json`. 
   2. Modify the configuration information appropriately. Information on the structure of the configuration file can be found [above](#scraper-configuration-file).
   3. Save your changes. 

4. Now, you'll have the opportunity to compile the project, thus getting your program. 
    ```
    cargo build --release
    ```

5. You should find the executable in the `/target/release` directory. Run the executable using the following command:
   ```
   ./webreg_scraper path_to_config_file
   ```
   where `path_to_config_file` should be replaced with the configuration file's name (full path). You may need to run
   `chmod u+x webreg_scraper` first so you can execute the program!

</details>

## Implementation
I'll only focus on the program's main feature -- tracking enrollment counts.

<details>
<summary>More Information</summary>
<br> 


### The Idea
At a very high level, the program (specifically, the scraper part) runs the following in an endless loop:
- Retrieve all possible courses.
- For each course:
    - Request data for that course.
    - Save that data to the CSV file.

We make use of [green threads](https://docs.rs/tokio/latest/tokio/task/index.html), managed by the Tokio runtime, to run through the above loop **concurrently** with other terms. In other words, we can say that we're running the above "program" multiple times at the same time. 

### Authentication
How do we run the above "program?" Well, the program makes use of an internal API that only UCSD students have access to (this is the same API that WebReg uses). To access the internal API, the program needs to be "logged into WebReg." This usually isn't hard; under most circumstances, you can just log in by "simulating" the login process. This is usually done in two ways:
- When sending the request, include an API key.
- Have the program make some API call with your login credentials and then retrieve the session cookies, which can then be used for future requests. 

Now, it's obvious that WebReg (or UCSD) isn't going to give me an API key. Regarding the second point, like with most schools, we have a 2FA system (Duo), which prevents me from just "simulating" the login process. More specifically, this is because due to two reasons:
- The big reason: most HTTP clients (like `reqwest`, which is what we're using) can only load the initial page source (what you would see when you view the page's source). However, Duo *dynamically* loads in a JavaScript `iframe`, which HTTP clients cannot render. Since it cannot render, the HTTP client can never "answer" it.
- A smaller reason: even if we *could* render the 2FA prompt, we would have to **manually** input the information (a 2FA code, for example).

Therefore, the solution is to just manually log in ourselves and then give the program the session cookies before running it.

### The Challenge
So, that simple solution doesn't seem too bad. However, the challenging part is keeping the program running 24/7. You see, WebReg restarts at around 4:30 AM every day. When WebReg restarts, every active session gets logged out. This includes the program.

The obvious solution would be to wake up at 4:30 AM every day and manually log the program in. However, this itself brings another problem -- waking up that early is hard for me.

This brings me to another solution -- there is additionally another project, creatively named [`webregautoin`](https://github.com/ewang2002/webreg_scraper/tree/master/webregautoin), which uses Node's [HTTP](https://nodejs.org/api/http.html) library to create a local web server which the wrapper can use. In particular, this local web server has one sole purpose: when new session cookies are needed to log into WebReg, the wrapper can make a request to the local API. The local API will then use [a headless Chrome browser](https://github.com/puppeteer/puppeteer) to log into WebReg and get the new cookies. Afterwards, those new cookies are returned back to the requester (the program), which the program can use to log back into WebReg and make more requests.

Why does *this* work but not the HTTP client? Well, a *headless browser* acts just like any browser, which means we can *load* Duo's `iframe` and thus authenticate. Of course, 2FA usually requires the end user to manually input a code. However, we can tell Duo to remember the current browser for 7 days*. Therefore, as long as the browser isn't closed\*\*, the headless browser can "bypass" the 2FA prompt and go straight to WebReg, which means we can get the session cookies.


\* - Note that if we couldn't do this, then we would have to resort to waking up every day at 4:30 AM. Oh, the horrors...

\*\* - Closing and reopening our headless browser resets everything due to the way most headless browsers are implemented.

</details>


## License
Everything in this repository is licensed under the MIT license.